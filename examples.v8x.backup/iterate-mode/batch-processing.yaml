# Batch Processing Example
#
# Large-scale file processing workflow demonstrating
# aggressive-timeout, reduced-parallelism, and skip-optional strategies

name: batch-file-processing
description: Process 1000+ files with AI model inference
version: 1.0.0

# Configuration
config:
  inputPath: ./data/input
  outputPath: ./data/output
  batchSize: 100
  parallelism: 5
  modelProvider: claude

# Workflow steps
steps:
  # Step 1: Scan input directory
  - name: scan-input-files
    description: Scan input directory and create file manifest
    provider: claude
    prompt: |
      Scan input directory for files to process:

      Directory: ./data/input
      File types: .txt, .md, .json, .csv

      Create manifest with:
      - File path
      - File size
      - Last modified timestamp
      - Estimated processing time
      - Priority (based on filename pattern)

      Return JSON manifest sorted by priority
    timeout: 30000
    required: true
    retryable: true

  # Step 2: Validate file manifest
  - name: validate-file-manifest
    description: Validate files are accessible and within size limits
    provider: claude
    prompt: |
      Validate file manifest from previous step:

      Validation rules:
      1. File exists and is readable
      2. File size < 10 MB
      3. File format is valid (check extension)
      4. No duplicate files
      5. Estimate total processing time

      Return validated manifest with pass/fail status per file
    timeout: 45000
    required: true
    retryable: true
    dependsOn: [scan-input-files]

  # Step 3: Process batch 1 (files 1-100)
  - name: process-batch-1
    description: Process first batch of 100 files
    provider: claude
    prompt: |
      Process files 1-100 from manifest:

      For each file:
      1. Read file content
      2. Run AI model inference (classification/extraction)
      3. Generate structured output
      4. Save to output directory
      5. Log processing time and status

      Return batch processing report: success, failed, skipped
    timeout: 300000  # 5 minutes for batch
    required: true
    retryable: true
    dependsOn: [validate-file-manifest]

  # Step 4: Process batch 2 (files 101-200)
  - name: process-batch-2
    description: Process second batch of 100 files
    provider: claude
    prompt: |
      Process files 101-200 from manifest:

      Same processing logic as batch 1
      Continue from where batch 1 left off
    timeout: 300000
    required: true
    retryable: true
    dependsOn: [process-batch-1]

  # Step 5: Process batch 3 (files 201-300)
  - name: process-batch-3
    description: Process third batch of 100 files
    provider: gemini  # Use different provider for load distribution
    prompt: |
      Process files 201-300 from manifest:

      Same processing logic as previous batches
    timeout: 300000
    required: true
    retryable: true
    dependsOn: [process-batch-2]

  # Step 6: Process batch 4 (files 301-400)
  - name: process-batch-4
    description: Process fourth batch of 100 files
    provider: openai
    prompt: |
      Process files 301-400 from manifest:

      Same processing logic as previous batches
    timeout: 300000
    required: true
    retryable: true
    dependsOn: [process-batch-3]

  # Step 7: Process remaining files (401+)
  - name: process-remaining-files
    description: Process all remaining files
    provider: claude
    prompt: |
      Process remaining files from manifest:

      Process in batches of 100
      Use same logic as previous batches
      Handle large files with extended timeout
    timeout: 600000  # 10 minutes for variable-size remainder
    required: true
    retryable: true
    dependsOn: [process-batch-4]

  # Step 8: Validate output files
  - name: validate-output-files
    description: Validate all output files were created correctly
    provider: claude
    prompt: |
      Validate output files:

      Checks:
      1. Output file exists for each input file
      2. Output file is valid JSON/format
      3. Required fields are present
      4. No corruption or truncation
      5. File sizes are reasonable

      Return validation report with success/failure counts
    timeout: 60000
    required: true
    retryable: true
    dependsOn: [process-remaining-files]

  # Step 9: Generate quality metrics (optional)
  - name: generate-quality-metrics
    description: Calculate data quality metrics across all processed files
    provider: claude
    prompt: |
      Generate quality metrics report:

      Metrics:
      1. Processing success rate
      2. Average processing time per file
      3. Model confidence scores (average, min, max)
      4. Error distribution (by error type)
      5. File size vs processing time correlation

      Return quality metrics dashboard JSON
    timeout: 45000
    required: false  # Optional analytics
    retryable: true
    dependsOn: [validate-output-files]

  # Step 10: Archive input files (optional)
  - name: archive-input-files
    description: Archive processed input files to S3
    provider: claude
    prompt: |
      Archive processed input files:

      Steps:
      1. Compress input files to .tar.gz
      2. Upload to S3: s3://archives/batch-{date}/
      3. Verify upload checksum
      4. Delete local input files (optional)

      Return archive summary with S3 URLs
    timeout: 120000
    required: false  # Optional archival
    retryable: true
    dependsOn: [validate-output-files]

  # Step 11: Send completion notification (optional)
  - name: send-completion-notification
    description: Notify team of batch processing completion
    provider: claude
    prompt: |
      Send batch processing completion notification:

      Include:
      - Total files processed
      - Success/failure breakdown
      - Total processing time
      - Average time per file
      - Quality metrics summary
      - Link to output directory

      Send to Slack #data-processing channel
    timeout: 15000
    required: false  # Optional notification
    retryable: true
    dependsOn: [generate-quality-metrics]

# Error handling
errorHandling:
  continueOnError: true  # Continue processing even if some batches fail
  capturePartialResults: true
  saveCheckpoints: true
  checkpointInterval: 2  # Checkpoint every 2 batches
  maxFailuresPerBatch: 10  # Skip batch if >10 files fail

# Resource limits
resources:
  maxMemory: 8GB
  maxCPU: 4
  maxOpenFiles: 1000
  maxConcurrentRequests: 5

# Monitoring
monitoring:
  logLevel: info
  metricsEnabled: true
  tracingEnabled: true
  alertOnFailure: true
  progressReporting: true
  reportInterval: 30000  # Report progress every 30 seconds

# Expected behavior with Iterate Mode:
#
# Iteration 1 (default strategy):
#   - scan-input-files: Succeeds
#   - validate-file-manifest: Succeeds
#   - process-batch-1: Timeout on large files
#   → Switches to aggressive-timeout strategy
#
# Iteration 2 (aggressive-timeout):
#   - Resumes from checkpoint (batch-1)
#   - process-batch-1: Succeeds with extended timeout
#   - process-batch-2: Timeout again on very large files
#   - process-batch-3: Runs in parallel with batch-2 (resource exhaustion)
#   → Switches to reduced-parallelism strategy
#
# Iteration 3 (reduced-parallelism + aggressive-timeout):
#   - Resumes from checkpoint (batch-2)
#   - process-batch-2: Succeeds sequentially
#   - process-batch-3: Succeeds
#   - process-batch-4: Succeeds
#   - process-remaining-files: Succeeds
#   - validate-output-files: Succeeds
#   - generate-quality-metrics: Fails (optional, can skip)
#   → Switches to skip-optional strategy
#
# Iteration 4 (skip-optional):
#   - Skips failed optional steps
#   - archive-input-files: Succeeds
#   - send-completion-notification: Succeeds
#   → Batch processing completes with partial success
#
# Total iterations: 4
# Expected duration: 20-30 minutes
# Expected cost: $3.00-5.00
#
# Recommended CLI options:
# ax workflow run examples/iterate-mode/batch-processing.yaml \
#   --iterate \
#   --max-iterations 10 \
#   --safety normal \
#   --max-cost 10.0 \
#   --checkpoint-interval 2 \
#   --verbose
