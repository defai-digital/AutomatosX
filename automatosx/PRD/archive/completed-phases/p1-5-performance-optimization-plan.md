# P1-5: Performance Optimization Implementation Plan

**AutomatosX v2 - Code Intelligence Engine**

**Created:** November 9, 2025
**Status:** Ready for Implementation
**Estimated Duration:** 7 days
**Priority:** High (completes P1 phase)

---

## Executive Summary

This document outlines a comprehensive performance optimization strategy for AutomatosX v2, targeting 2-3x improvements in indexing speed, memory efficiency, and query latency through parallel processing, intelligent caching, and incremental analysis.

### Current Performance Baseline

| Metric | Current Value | Target Value | Improvement |
|--------|--------------|--------------|-------------|
| **Indexing Speed** | ~10 files/sec | >20 files/sec | 2x faster |
| **Query Latency (cached)** | <1ms | <1ms | Maintain |
| **Query Latency (uncached)** | <5ms (P95) | <3ms (P95) | 1.7x faster |
| **Cache Hit Rate** | 60% | 80% | +33% |
| **Memory Usage (1000 files)** | ~200MB | <500MB | 2.5x headroom |
| **Test Coverage** | 100% (226/226) | 100% | Maintain |

### Optimization Targets

1. **Parallel File Processing** (Days 1-2)
   - Worker thread pool for Tree-sitter parsing
   - Concurrent quality analysis
   - Expected: 2-3x indexing speedup

2. **AST Caching** (Day 3)
   - LRU cache for parsed ASTs (max 100 files)
   - File hash-based invalidation
   - Expected: 50% reduction in parse time for hot files

3. **Incremental Analysis** (Days 4-5)
   - Function-level change detection
   - Selective re-analysis of modified functions only
   - Expected: 5-10x speedup for small changes

4. **Memory Optimization** (Day 6)
   - Streaming file processing
   - Chunk-based analysis with memory pooling
   - Expected: 50% memory reduction

5. **Performance Testing & Validation** (Day 7)
   - Comprehensive benchmarks
   - Performance regression tests
   - Final optimization report

---

## Current Architecture Analysis

### Bottleneck Identification

Based on code review of `FileService.ts`, `QualityService.ts`, and `ParserRegistry.ts`:

#### 1. **Sequential File Processing** (PRIMARY BOTTLENECK)

**Location:** `FileService.indexFile()` (lines 128-194)

**Issue:**
```typescript
// Current implementation processes files one at a time
indexFile(path: string, content: string): IndexResult {
  const parseResult = this.parserRegistry.parse(content, path);  // BLOCKING
  const chunkingResult = this.chunkingService.chunkFile(content, parseResult.symbols);
  const result = transaction(() => { /* ... */ });
  return result;
}
```

**Impact:**
- Tree-sitter parsing is CPU-intensive (avg 50-100ms per file)
- Single-threaded execution leaves cores idle
- I/O operations block CPU work

**Solution:** Worker thread pool with parallel parsing

---

#### 2. **No AST Caching** (SECONDARY BOTTLENECK)

**Location:** `ParserRegistry.parse()` (line 232)

**Issue:**
```typescript
parse(content: string, filePath: string): ParseResult {
  const parser = this.getParserByPath(filePath);
  return parser.parse(content);  // Re-parses every time, no caching
}
```

**Impact:**
- Repeated parsing of unchanged files (common in file watching)
- Quality analysis re-runs full parse for same file
- Wasted CPU cycles on unchanged code

**Solution:** LRU cache keyed by (filePath + contentHash)

---

#### 3. **Full File Re-analysis on Change** (TERTIARY BOTTLENECK)

**Location:** `FileService.reindexFile()` (lines 203-274)

**Issue:**
```typescript
reindexFile(path: string, content: string): IndexResult {
  // Deletes ALL symbols and chunks
  this.symbolDAO.deleteByFileId(existingFile.id);
  this.chunkDAO.deleteByFileId(existingFile.id);

  // Re-parses ENTIRE file
  const parseResult = this.parserRegistry.parse(content, path);

  // Re-inserts ALL symbols
  const symbolIds = this.symbolDAO.insertBatch(symbolInputs);
}
```

**Impact:**
- Small function change triggers full file re-analysis
- Database thrashing (delete + insert entire symbol table)
- Unnecessary work for unchanged functions

**Solution:** Function-level diff, selective re-analysis

---

#### 4. **Quality Analysis Sequential Processing**

**Location:** `QualityService.analyzeProject()` (lines 108-141)

**Issue:**
```typescript
for (const { filePath, language } of files) {
  try {
    const report = await this.analyzeFile(filePath, language);  // SEQUENTIAL
    fileReports.push(report);
  } catch (error) { /* ... */ }
}
```

**Impact:**
- Files analyzed one by one
- Complexity analysis is CPU-bound but not parallelized
- Large projects have long analysis times

**Solution:** Parallel analysis with worker pool

---

#### 5. **Memory: No Streaming for Large Files**

**Location:** `FileService.indexFile()` and `QualityService.analyzeFile()`

**Issue:**
```typescript
// Loads entire file into memory
const content = await fs.readFile(filePath, 'utf-8');

// Parses entire file at once
const parseResult = this.parserRegistry.parse(content, path);

// Creates all chunks in memory
const chunkingResult = this.chunkingService.chunkFile(content, parseResult.symbols);
```

**Impact:**
- Large files (>1MB) consume significant memory
- All chunks created before any are saved
- Memory spikes for large codebases

**Solution:** Streaming chunk processing, batch inserts

---

## Implementation Plan

### Phase 1: Parallel File Processing (Days 1-2)

**Objective:** Achieve 2x+ indexing speedup through worker threads

#### 1.1: Create Worker Thread Infrastructure

**File:** `src/performance/WorkerPool.ts` (NEW)

**Design:**
```typescript
import { Worker } from 'worker_threads';
import * as os from 'os';

export interface WorkerTask<T, R> {
  id: string;
  data: T;
  resolve: (result: R) => void;
  reject: (error: Error) => void;
}

export interface WorkerPoolOptions {
  maxWorkers?: number;  // Default: CPU count - 1
  taskTimeout?: number;  // Default: 30000ms
}

export class WorkerPool<T, R> {
  private workers: Worker[] = [];
  private taskQueue: WorkerTask<T, R>[] = [];
  private activeTasks: Map<string, WorkerTask<T, R>> = new Map();
  private availableWorkers: Worker[] = [];

  constructor(
    private workerScript: string,
    private options: WorkerPoolOptions = {}
  ) {
    const maxWorkers = options.maxWorkers || Math.max(1, os.cpus().length - 1);
    this.initializeWorkers(maxWorkers);
  }

  private initializeWorkers(count: number): void {
    for (let i = 0; i < count; i++) {
      const worker = new Worker(this.workerScript);

      worker.on('message', (result: { taskId: string; result: R; error?: Error }) => {
        const task = this.activeTasks.get(result.taskId);
        if (!task) return;

        this.activeTasks.delete(result.taskId);
        this.availableWorkers.push(worker);

        if (result.error) {
          task.reject(result.error);
        } else {
          task.resolve(result.result);
        }

        // Process next task if any
        this.processNextTask();
      });

      worker.on('error', (error) => {
        console.error('Worker error:', error);
        // Handle worker crash - restart worker
        this.restartWorker(worker);
      });

      this.workers.push(worker);
      this.availableWorkers.push(worker);
    }
  }

  async execute(data: T): Promise<R> {
    return new Promise((resolve, reject) => {
      const task: WorkerTask<T, R> = {
        id: `task-${Date.now()}-${Math.random()}`,
        data,
        resolve,
        reject,
      };

      this.taskQueue.push(task);
      this.processNextTask();
    });
  }

  private processNextTask(): void {
    if (this.taskQueue.length === 0 || this.availableWorkers.length === 0) {
      return;
    }

    const task = this.taskQueue.shift()!;
    const worker = this.availableWorkers.shift()!;

    this.activeTasks.set(task.id, task);

    // Send task to worker
    worker.postMessage({
      taskId: task.id,
      data: task.data,
    });

    // Set timeout
    setTimeout(() => {
      if (this.activeTasks.has(task.id)) {
        this.activeTasks.delete(task.id);
        task.reject(new Error('Task timeout'));
        this.restartWorker(worker);
      }
    }, this.options.taskTimeout || 30000);
  }

  private restartWorker(worker: Worker): void {
    const index = this.workers.indexOf(worker);
    if (index !== -1) {
      worker.terminate();
      const newWorker = new Worker(this.workerScript);
      this.workers[index] = newWorker;
      this.availableWorkers.push(newWorker);
    }
  }

  async shutdown(): Promise<void> {
    for (const worker of this.workers) {
      await worker.terminate();
    }
    this.workers = [];
    this.availableWorkers = [];
  }

  getStats(): {
    totalWorkers: number;
    availableWorkers: number;
    queuedTasks: number;
    activeTasks: number;
  } {
    return {
      totalWorkers: this.workers.length,
      availableWorkers: this.availableWorkers.length,
      queuedTasks: this.taskQueue.length,
      activeTasks: this.activeTasks.size,
    };
  }
}
```

#### 1.2: Create Parsing Worker

**File:** `src/performance/workers/parsingWorker.ts` (NEW)

```typescript
import { parentPort } from 'worker_threads';
import { getParserRegistry } from '../../parser/ParserRegistry.js';
import type { ParseResult } from '../../parser/LanguageParser.js';

interface ParsingTask {
  taskId: string;
  data: {
    content: string;
    filePath: string;
  };
}

interface ParsingResult {
  taskId: string;
  result?: ParseResult;
  error?: Error;
}

// Initialize parser registry in worker
const parserRegistry = getParserRegistry();

parentPort?.on('message', (task: ParsingTask) => {
  try {
    const result = parserRegistry.parse(task.data.content, task.data.filePath);

    parentPort?.postMessage({
      taskId: task.taskId,
      result,
    } as ParsingResult);
  } catch (error) {
    parentPort?.postMessage({
      taskId: task.taskId,
      error: error instanceof Error ? error : new Error(String(error)),
    } as ParsingResult);
  }
});
```

#### 1.3: Create Quality Analysis Worker

**File:** `src/performance/workers/qualityWorker.ts` (NEW)

```typescript
import { parentPort } from 'worker_threads';
import { ComplexityAnalyzer } from '../../analytics/quality/ComplexityAnalyzer.js';
import { MaintainabilityCalculator } from '../../analytics/quality/MaintainabilityCalculator.js';
import type { Language } from '../../types/index.js';
import type { FileComplexity } from '../../analytics/quality/ComplexityAnalyzer.js';
import type { MaintainabilityMetrics } from '../../analytics/quality/MaintainabilityCalculator.js';

interface QualityTask {
  taskId: string;
  data: {
    filePath: string;
    content: string;
    language: Language;
  };
}

interface QualityResult {
  taskId: string;
  result?: {
    complexity: FileComplexity;
    maintainability: MaintainabilityMetrics;
  };
  error?: Error;
}

// Initialize analyzers in worker
const complexityAnalyzer = new ComplexityAnalyzer();
const maintainabilityCalculator = new MaintainabilityCalculator();

parentPort?.on('message', async (task: QualityTask) => {
  try {
    const complexity = await complexityAnalyzer.analyzeFile(
      task.data.filePath,
      task.data.content,
      task.data.language
    );

    const maintainability = maintainabilityCalculator.calculateMaintainability(complexity);

    parentPort?.postMessage({
      taskId: task.taskId,
      result: {
        complexity,
        maintainability,
      },
    } as QualityResult);
  } catch (error) {
    parentPort?.postMessage({
      taskId: task.taskId,
      error: error instanceof Error ? error : new Error(String(error)),
    } as QualityResult);
  }
});
```

#### 1.4: Integrate Worker Pool into FileService

**File:** `src/services/FileService.ts` (MODIFY)

**Changes:**
1. Add worker pool initialization
2. Create parallel batch indexing method
3. Maintain backwards compatibility with single-file API

```typescript
// Add to imports
import { WorkerPool } from '../performance/WorkerPool.js';
import * as path from 'path';

export class FileService {
  // Add worker pool fields
  private parsingWorkerPool?: WorkerPool<
    { content: string; filePath: string },
    ParseResult
  >;

  constructor(options?: { enableParallel?: boolean }) {
    // ... existing initialization ...

    // Initialize worker pool if parallel mode enabled
    if (options?.enableParallel !== false) {
      this.parsingWorkerPool = new WorkerPool(
        path.join(__dirname, '../performance/workers/parsingWorker.js'),
        { maxWorkers: os.cpus().length - 1 }
      );
    }
  }

  /**
   * Index multiple files in parallel
   *
   * @param files - Array of { path, content } to index
   * @returns Array of index results
   */
  async indexFilesParallel(
    files: Array<{ path: string; content: string }>
  ): Promise<IndexResult[]> {
    if (!this.parsingWorkerPool) {
      // Fallback to sequential if worker pool not available
      return Promise.all(files.map(f => Promise.resolve(this.indexFile(f.path, f.content))));
    }

    const startTime = performance.now();

    // Parse all files in parallel using worker pool
    const parsePromises = files.map(file =>
      this.parsingWorkerPool!.execute({
        content: file.content,
        filePath: file.path,
      })
    );

    const parseResults = await Promise.all(parsePromises);

    // Create chunks for all files (fast, no I/O)
    const chunkingResults = files.map((file, i) =>
      this.chunkingService.chunkFile(file.content, parseResults[i].symbols)
    );

    // Batch insert all files into database (single transaction)
    const results = transaction(() => {
      const indexResults: IndexResult[] = [];

      for (let i = 0; i < files.length; i++) {
        const file = files[i];
        const parseResult = parseResults[i];
        const chunkingResult = chunkingResults[i];

        // Insert file
        const fileId = this.fileDAO.insert({
          path: file.path,
          content: file.content,
          language: this.parserRegistry.getLanguageForPath(file.path),
        });

        // Batch insert symbols
        const symbolInputs: SymbolInput[] = parseResult.symbols.map(symbol => ({
          file_id: fileId,
          name: symbol.name,
          kind: symbol.kind,
          line: symbol.line,
          column: symbol.column,
          end_line: symbol.endLine,
          end_column: symbol.endColumn,
        }));
        const symbolIds = this.symbolDAO.insertBatch(symbolInputs);

        // Batch insert chunks
        const symbolIdMap = new Map<number, number>();
        parseResult.symbols.forEach((symbol, idx) => {
          symbolIdMap.set(symbol.line, symbolIds[idx]);
        });

        const chunkInputs: ChunkInput[] = chunkingResult.chunks.map(chunk => ({
          file_id: fileId,
          content: chunk.content,
          start_line: chunk.start_line,
          end_line: chunk.end_line,
          chunk_type: chunk.chunk_type,
          symbol_id: symbolIdMap.get(chunk.start_line),
        }));
        this.chunkDAO.insertBatch(chunkInputs);

        indexResults.push({
          fileId,
          symbolCount: parseResult.symbols.length,
          chunkCount: chunkingResult.chunks.length,
          parseTime: parseResult.parseTime,
          totalTime: 0, // Will be set below
        });
      }

      return indexResults;
    });

    const endTime = performance.now();
    const totalTime = endTime - startTime;

    // Update total time for all results
    results.forEach(r => (r.totalTime = totalTime));

    // Invalidate cache
    this.invalidateCache();

    return results;
  }

  /**
   * Shutdown worker pool (call on app exit)
   */
  async shutdown(): Promise<void> {
    if (this.parsingWorkerPool) {
      await this.parsingWorkerPool.shutdown();
    }
  }
}
```

#### 1.5: Integrate Worker Pool into QualityService

**File:** `src/analytics/quality/QualityService.ts` (MODIFY)

```typescript
import { WorkerPool } from '../../performance/WorkerPool.js';
import * as os from 'os';

export class QualityService {
  private qualityWorkerPool?: WorkerPool<
    { filePath: string; content: string; language: Language },
    { complexity: FileComplexity; maintainability: MaintainabilityMetrics }
  >;

  constructor(options?: QualityServiceOptions & { enableParallel?: boolean }) {
    // ... existing initialization ...

    // Initialize worker pool if parallel mode enabled
    if (options?.enableParallel !== false) {
      this.qualityWorkerPool = new WorkerPool(
        path.join(__dirname, '../../performance/workers/qualityWorker.js'),
        { maxWorkers: os.cpus().length - 1 }
      );
    }
  }

  /**
   * Analyze quality for entire project (PARALLEL VERSION)
   */
  async analyzeProject(projectPath: string, languages: Language[]): Promise<ProjectQualityReport> {
    // Find all relevant files
    const files = await this.findSourceFiles(projectPath, languages);

    if (!this.qualityWorkerPool) {
      // Fallback to sequential processing
      const fileReports: QualityReport[] = [];
      for (const { filePath, language } of files) {
        try {
          const report = await this.analyzeFile(filePath, language);
          fileReports.push(report);
        } catch (error) {
          console.error(`Failed to analyze ${filePath}:`, error);
        }
      }

      return {
        projectPath,
        timestamp: new Date(),
        fileReports,
        aggregateMetrics: this.calculateAggregateMetrics(fileReports),
        trends: { complexityTrend: 'stable', maintainabilityTrend: 'stable', debtTrend: 'stable' },
      };
    }

    // Read all files first
    const fileContents = await Promise.all(
      files.map(async ({ filePath, language }) => ({
        filePath,
        language,
        content: await fs.readFile(filePath, 'utf-8'),
      }))
    );

    // Analyze all files in parallel
    const analysisPromises = fileContents.map(({ filePath, language, content }) =>
      this.qualityWorkerPool!.execute({ filePath, content, language })
        .then(({ complexity, maintainability }) => ({
          filePath,
          language,
          timestamp: new Date(),
          complexity,
          maintainability,
          summary: this.generateSummary(complexity, maintainability),
        }))
        .catch(error => {
          console.error(`Failed to analyze ${filePath}:`, error);
          return null;
        })
    );

    const results = await Promise.all(analysisPromises);
    const fileReports = results.filter((r): r is QualityReport => r !== null);

    return {
      projectPath,
      timestamp: new Date(),
      fileReports,
      aggregateMetrics: this.calculateAggregateMetrics(fileReports),
      trends: { complexityTrend: 'stable', maintainabilityTrend: 'stable', debtTrend: 'stable' },
    };
  }

  async shutdown(): Promise<void> {
    if (this.qualityWorkerPool) {
      await this.qualityWorkerPool.shutdown();
    }
  }
}
```

#### 1.6: Performance Testing

**File:** `src/__tests__/performance/parallel-processing.test.ts` (NEW)

```typescript
import { describe, it, expect, beforeAll, afterAll } from 'vitest';
import { FileService } from '../../services/FileService.js';
import { QualityService } from '../../analytics/quality/QualityService.js';
import * as fs from 'fs/promises';
import * as path from 'path';

describe('Parallel Processing Performance', () => {
  let fileService: FileService;
  let qualityService: QualityService;

  beforeAll(() => {
    fileService = new FileService({ enableParallel: true });
    qualityService = new QualityService({ enableParallel: true });
  });

  afterAll(async () => {
    await fileService.shutdown();
    await qualityService.shutdown();
  });

  it('should index files faster in parallel vs sequential', async () => {
    // Create test files
    const testFiles = Array.from({ length: 20 }, (_, i) => ({
      path: `/test/file${i}.ts`,
      content: `
        export function test${i}() {
          const x = ${i};
          return x * 2;
        }
      `,
    }));

    // Sequential timing
    const seqStart = performance.now();
    for (const file of testFiles) {
      fileService.indexFile(file.path, file.content);
    }
    const seqTime = performance.now() - seqStart;

    // Parallel timing
    const parStart = performance.now();
    await fileService.indexFilesParallel(testFiles);
    const parTime = performance.now() - parStart;

    console.log(`Sequential: ${seqTime.toFixed(2)}ms, Parallel: ${parTime.toFixed(2)}ms`);
    console.log(`Speedup: ${(seqTime / parTime).toFixed(2)}x`);

    // Expect at least 1.5x speedup (conservative)
    expect(parTime).toBeLessThan(seqTime / 1.5);
  });

  it('should analyze project faster in parallel', async () => {
    const testDir = '/tmp/test-project';
    await fs.mkdir(testDir, { recursive: true });

    // Create test files
    for (let i = 0; i < 10; i++) {
      await fs.writeFile(
        path.join(testDir, `file${i}.ts`),
        `export function test${i}() { return ${i}; }`
      );
    }

    const start = performance.now();
    const report = await qualityService.analyzeProject(testDir, ['typescript']);
    const time = performance.now() - start;

    expect(report.fileReports.length).toBe(10);
    expect(time).toBeLessThan(5000); // Should complete in under 5 seconds

    // Cleanup
    await fs.rm(testDir, { recursive: true });
  });
});
```

**Expected Results:**
- 2-3x speedup for 20+ file indexing
- Linear scaling with CPU cores (tested on 4-core, 8-core machines)
- No race conditions or data corruption

---

### Phase 2: AST Caching (Day 3)

**Objective:** Reduce repeated parsing overhead by 50%+

#### 2.1: Create AST Cache

**File:** `src/cache/ASTCache.ts` (NEW)

```typescript
import * as crypto from 'crypto';
import type { ParseResult } from '../parser/LanguageParser.js';

export interface CacheEntry {
  ast: ParseResult;
  hash: string;
  filePath: string;
  timestamp: number;
  hits: number;
}

export interface ASTCacheOptions {
  maxSize?: number;  // Default: 100 files
  ttl?: number;      // Default: 1 hour (3600000ms)
}

export interface ASTCacheStats {
  size: number;
  maxSize: number;
  hits: number;
  misses: number;
  hitRate: number;
  evictions: number;
}

/**
 * LRU Cache for parsed ASTs
 * Keyed by file path + content hash
 */
export class ASTCache {
  private cache: Map<string, CacheEntry> = new Map();
  private lruOrder: string[] = [];
  private maxSize: number;
  private ttl: number;
  private hits = 0;
  private misses = 0;
  private evictions = 0;

  constructor(options: ASTCacheOptions = {}) {
    this.maxSize = options.maxSize || 100;
    this.ttl = options.ttl || 3600000; // 1 hour
  }

  /**
   * Compute content hash for cache key
   */
  private computeHash(content: string): string {
    return crypto.createHash('sha256').update(content).digest('hex').slice(0, 16);
  }

  /**
   * Generate cache key from file path and content
   */
  private getCacheKey(filePath: string, content: string): string {
    const hash = this.computeHash(content);
    return `${filePath}:${hash}`;
  }

  /**
   * Get cached AST if available and valid
   */
  get(filePath: string, content: string): ParseResult | null {
    const key = this.getCacheKey(filePath, content);
    const entry = this.cache.get(key);

    if (!entry) {
      this.misses++;
      return null;
    }

    // Check TTL
    const age = Date.now() - entry.timestamp;
    if (age > this.ttl) {
      this.cache.delete(key);
      this.lruOrder = this.lruOrder.filter(k => k !== key);
      this.misses++;
      return null;
    }

    // Update LRU order (move to end)
    this.lruOrder = this.lruOrder.filter(k => k !== key);
    this.lruOrder.push(key);

    // Update stats
    entry.hits++;
    this.hits++;

    return entry.ast;
  }

  /**
   * Store AST in cache
   */
  set(filePath: string, content: string, ast: ParseResult): void {
    const key = this.getCacheKey(filePath, content);
    const hash = this.computeHash(content);

    // If already exists, update
    if (this.cache.has(key)) {
      const entry = this.cache.get(key)!;
      entry.ast = ast;
      entry.timestamp = Date.now();

      // Move to end of LRU
      this.lruOrder = this.lruOrder.filter(k => k !== key);
      this.lruOrder.push(key);
      return;
    }

    // Evict LRU entry if cache is full
    if (this.cache.size >= this.maxSize) {
      const evictKey = this.lruOrder.shift();
      if (evictKey) {
        this.cache.delete(evictKey);
        this.evictions++;
      }
    }

    // Add new entry
    this.cache.set(key, {
      ast,
      hash,
      filePath,
      timestamp: Date.now(),
      hits: 0,
    });
    this.lruOrder.push(key);
  }

  /**
   * Invalidate cache for specific file
   */
  invalidate(filePath: string): void {
    // Remove all entries for this file path (all content hashes)
    const keysToRemove: string[] = [];
    for (const [key, entry] of this.cache.entries()) {
      if (entry.filePath === filePath) {
        keysToRemove.push(key);
      }
    }

    for (const key of keysToRemove) {
      this.cache.delete(key);
      this.lruOrder = this.lruOrder.filter(k => k !== key);
    }
  }

  /**
   * Clear entire cache
   */
  clear(): void {
    this.cache.clear();
    this.lruOrder = [];
    this.hits = 0;
    this.misses = 0;
    this.evictions = 0;
  }

  /**
   * Get cache statistics
   */
  stats(): ASTCacheStats {
    const total = this.hits + this.misses;
    return {
      size: this.cache.size,
      maxSize: this.maxSize,
      hits: this.hits,
      misses: this.misses,
      hitRate: total > 0 ? this.hits / total : 0,
      evictions: this.evictions,
    };
  }

  /**
   * Get top N most accessed files
   */
  getTopFiles(n: number = 10): Array<{ filePath: string; hits: number }> {
    const entries = Array.from(this.cache.values());
    entries.sort((a, b) => b.hits - a.hits);
    return entries.slice(0, n).map(e => ({
      filePath: e.filePath,
      hits: e.hits,
    }));
  }
}
```

#### 2.2: Integrate AST Cache into ParserRegistry

**File:** `src/parser/ParserRegistry.ts` (MODIFY)

```typescript
import { ASTCache } from '../cache/ASTCache.js';

export class ParserRegistry {
  private astCache: ASTCache;

  constructor(options?: { enableCache?: boolean; cacheSize?: number }) {
    // ... existing initialization ...

    // Initialize AST cache
    this.astCache = new ASTCache({
      maxSize: options?.cacheSize || 100,
      ttl: 3600000, // 1 hour
    });
  }

  /**
   * Parse source code with caching
   */
  parse(content: string, filePath: string): ParseResult {
    // Check cache first
    const cached = this.astCache.get(filePath, content);
    if (cached) {
      return cached;
    }

    // Cache miss - parse file
    const parser = this.getParserByPath(filePath);
    if (!parser) {
      const ext = extname(filePath);
      throw new Error(`No parser registered for file extension: ${ext}`);
    }

    const result = parser.parse(content);

    // Store in cache
    this.astCache.set(filePath, content, result);

    return result;
  }

  /**
   * Invalidate cache for specific file
   */
  invalidateCache(filePath: string): void {
    this.astCache.invalidate(filePath);
  }

  /**
   * Get AST cache statistics
   */
  getCacheStats() {
    return this.astCache.stats();
  }
}
```

#### 2.3: Update FileService to Invalidate Cache on Changes

**File:** `src/services/FileService.ts` (MODIFY)

```typescript
reindexFile(path: string, content: string): IndexResult {
  // Invalidate AST cache for this file
  this.parserRegistry.invalidateCache(path);

  // ... existing reindex logic ...
}
```

#### 2.4: Testing

**File:** `src/__tests__/cache/ASTCache.test.ts` (NEW)

```typescript
import { describe, it, expect, beforeEach } from 'vitest';
import { ASTCache } from '../../cache/ASTCache.js';
import type { ParseResult } from '../../parser/LanguageParser.js';

describe('ASTCache', () => {
  let cache: ASTCache;
  let mockAST: ParseResult;

  beforeEach(() => {
    cache = new ASTCache({ maxSize: 3, ttl: 1000 });
    mockAST = {
      symbols: [{ name: 'test', kind: 'function', line: 1, column: 0 }],
      parseTime: 10,
    };
  });

  it('should cache and retrieve AST', () => {
    cache.set('/test.ts', 'const x = 1;', mockAST);
    const result = cache.get('/test.ts', 'const x = 1;');

    expect(result).toEqual(mockAST);
    expect(cache.stats().hits).toBe(1);
    expect(cache.stats().misses).toBe(0);
  });

  it('should return null for cache miss', () => {
    const result = cache.get('/test.ts', 'const x = 1;');

    expect(result).toBeNull();
    expect(cache.stats().misses).toBe(1);
  });

  it('should invalidate on content change', () => {
    cache.set('/test.ts', 'const x = 1;', mockAST);
    expect(cache.get('/test.ts', 'const x = 1;')).toEqual(mockAST);

    // Different content = different hash = cache miss
    expect(cache.get('/test.ts', 'const x = 2;')).toBeNull();
  });

  it('should evict LRU entry when full', () => {
    cache.set('/file1.ts', 'code1', mockAST);
    cache.set('/file2.ts', 'code2', mockAST);
    cache.set('/file3.ts', 'code3', mockAST);

    // Access file1 to make it more recent
    cache.get('/file1.ts', 'code1');

    // Add file4 - should evict file2 (LRU)
    cache.set('/file4.ts', 'code4', mockAST);

    expect(cache.get('/file1.ts', 'code1')).toEqual(mockAST);
    expect(cache.get('/file2.ts', 'code2')).toBeNull();
    expect(cache.get('/file3.ts', 'code3')).toEqual(mockAST);
    expect(cache.get('/file4.ts', 'code4')).toEqual(mockAST);

    expect(cache.stats().evictions).toBe(1);
  });

  it('should invalidate specific file', () => {
    cache.set('/test.ts', 'const x = 1;', mockAST);
    cache.invalidate('/test.ts');

    expect(cache.get('/test.ts', 'const x = 1;')).toBeNull();
  });

  it('should respect TTL', async () => {
    cache.set('/test.ts', 'const x = 1;', mockAST);

    // Wait for TTL to expire
    await new Promise(resolve => setTimeout(resolve, 1100));

    expect(cache.get('/test.ts', 'const x = 1;')).toBeNull();
  });
});
```

**Expected Results:**
- 50-80% cache hit rate for typical workflows
- <1ms cache lookup time
- Proper LRU eviction and TTL expiration

---

### Phase 3: Incremental Analysis (Days 4-5)

**Objective:** 5-10x speedup for small code changes

#### 3.1: Function-Level Change Detection

**File:** `src/services/IncrementalAnalyzer.ts` (NEW)

```typescript
import type { ParseResult, Symbol } from '../parser/LanguageParser.js';
import type { FileDAO } from '../database/dao/FileDAO.js';
import type { SymbolDAO } from '../database/dao/SymbolDAO.js';
import * as crypto from 'crypto';

export interface FunctionDiff {
  added: Symbol[];
  removed: Symbol[];
  modified: Symbol[];
  unchanged: Symbol[];
}

export interface IncrementalResult {
  needsFullReindex: boolean;
  diff?: FunctionDiff;
  affectedSymbolIds?: number[];
}

/**
 * Detects function-level changes for incremental re-analysis
 */
export class IncrementalAnalyzer {
  constructor(
    private fileDAO: FileDAO,
    private symbolDAO: SymbolDAO
  ) {}

  /**
   * Compute content hash for symbol
   */
  private computeSymbolHash(symbol: Symbol, content: string): string {
    // Extract symbol content from file
    const lines = content.split('\n');
    const symbolContent = lines
      .slice(symbol.line - 1, (symbol.endLine || symbol.line))
      .join('\n');

    return crypto.createHash('sha256')
      .update(symbolContent)
      .digest('hex')
      .slice(0, 16);
  }

  /**
   * Analyze changes between old and new file versions
   */
  analyzeChanges(
    filePath: string,
    newContent: string,
    newSymbols: Symbol[]
  ): IncrementalResult {
    // Get existing file and symbols from database
    const existingFile = this.fileDAO.findByPath(filePath);
    if (!existingFile) {
      return { needsFullReindex: true };
    }

    const existingSymbols = this.symbolDAO.findByFileId(existingFile.id);

    // If symbol count changed significantly, do full reindex
    const symbolCountChange = Math.abs(
      newSymbols.length - existingSymbols.length
    );
    if (symbolCountChange > newSymbols.length * 0.3) {
      // >30% change in symbol count
      return { needsFullReindex: true };
    }

    // Build maps for comparison
    const oldSymbolMap = new Map<string, typeof existingSymbols[0]>();
    for (const sym of existingSymbols) {
      const key = `${sym.kind}:${sym.name}:${sym.line}`;
      oldSymbolMap.set(key, sym);
    }

    const newSymbolMap = new Map<string, Symbol>();
    for (const sym of newSymbols) {
      const key = `${sym.kind}:${sym.name}:${sym.line}`;
      newSymbolMap.set(key, sym);
    }

    // Detect changes
    const added: Symbol[] = [];
    const removed: Symbol[] = [];
    const modified: Symbol[] = [];
    const unchanged: Symbol[] = [];

    // Find added and modified symbols
    for (const [key, newSym] of newSymbolMap.entries()) {
      const oldSym = oldSymbolMap.get(key);

      if (!oldSym) {
        added.push(newSym);
      } else {
        // Compare hashes to detect modifications
        const oldHash = this.computeSymbolHash(
          {
            name: oldSym.name,
            kind: oldSym.kind,
            line: oldSym.line,
            column: oldSym.column,
            endLine: oldSym.end_line,
            endColumn: oldSym.end_column,
          },
          existingFile.content
        );
        const newHash = this.computeSymbolHash(newSym, newContent);

        if (oldHash !== newHash) {
          modified.push(newSym);
        } else {
          unchanged.push(newSym);
        }
      }
    }

    // Find removed symbols
    for (const [key, oldSym] of oldSymbolMap.entries()) {
      if (!newSymbolMap.has(key)) {
        removed.push({
          name: oldSym.name,
          kind: oldSym.kind,
          line: oldSym.line,
          column: oldSym.column,
          endLine: oldSym.end_line,
          endColumn: oldSym.end_column,
        });
      }
    }

    // If too many changes, do full reindex
    const changeRatio = (added.length + removed.length + modified.length) / newSymbols.length;
    if (changeRatio > 0.5) {
      return { needsFullReindex: true };
    }

    // Get affected symbol IDs for deletion
    const affectedSymbolIds = removed.map(sym => {
      const key = `${sym.kind}:${sym.name}:${sym.line}`;
      return oldSymbolMap.get(key)?.id;
    }).filter((id): id is number => id !== undefined);

    return {
      needsFullReindex: false,
      diff: { added, removed, modified, unchanged },
      affectedSymbolIds,
    };
  }
}
```

#### 3.2: Integrate Incremental Analysis into FileService

**File:** `src/services/FileService.ts` (MODIFY)

```typescript
import { IncrementalAnalyzer } from './IncrementalAnalyzer.js';

export class FileService {
  private incrementalAnalyzer: IncrementalAnalyzer;

  constructor(options?: { enableParallel?: boolean; enableIncremental?: boolean }) {
    // ... existing initialization ...

    this.incrementalAnalyzer = new IncrementalAnalyzer(this.fileDAO, this.symbolDAO);
  }

  /**
   * Re-index file with incremental optimization
   */
  reindexFile(path: string, content: string): IndexResult {
    const startTime = performance.now();

    // Parse new content
    const parseResult = this.parserRegistry.parse(content, path);

    // Analyze changes
    const analysisResult = this.incrementalAnalyzer.analyzeChanges(
      path,
      content,
      parseResult.symbols
    );

    // If needs full reindex, use original logic
    if (analysisResult.needsFullReindex) {
      return this.reindexFileFull(path, content, parseResult);
    }

    // Otherwise, do incremental update
    return this.reindexFileIncremental(
      path,
      content,
      parseResult,
      analysisResult
    );
  }

  /**
   * Full file reindex (original logic)
   */
  private reindexFileFull(
    path: string,
    content: string,
    parseResult: ParseResult
  ): IndexResult {
    // ... existing reindexFile logic ...
  }

  /**
   * Incremental file reindex (only changed symbols)
   */
  private reindexFileIncremental(
    path: string,
    content: string,
    parseResult: ParseResult,
    analysisResult: IncrementalResult
  ): IndexResult {
    const startTime = performance.now();
    const { diff, affectedSymbolIds } = analysisResult;

    if (!diff) {
      throw new Error('Diff required for incremental reindex');
    }

    // Create chunks
    const chunkingResult = this.chunkingService.chunkFile(content, parseResult.symbols);

    // Update in transaction
    const result = transaction(() => {
      // Get existing file
      const existingFile = this.fileDAO.findByPath(path);
      if (!existingFile) {
        throw new Error(`File not found: ${path}`);
      }

      // Update file content
      this.fileDAO.update(existingFile.id, { content });

      // Delete removed symbols and their chunks
      if (affectedSymbolIds && affectedSymbolIds.length > 0) {
        for (const symbolId of affectedSymbolIds) {
          this.chunkDAO.deleteBySymbolId(symbolId);
          this.symbolDAO.delete(symbolId);
        }
      }

      // Insert added symbols
      const addedInputs: SymbolInput[] = diff.added.map(symbol => ({
        file_id: existingFile.id,
        name: symbol.name,
        kind: symbol.kind,
        line: symbol.line,
        column: symbol.column,
        end_line: symbol.endLine,
        end_column: symbol.endColumn,
      }));
      const addedIds = this.symbolDAO.insertBatch(addedInputs);

      // Update modified symbols
      const modifiedInputs: SymbolInput[] = diff.modified.map(symbol => ({
        file_id: existingFile.id,
        name: symbol.name,
        kind: symbol.kind,
        line: symbol.line,
        column: symbol.column,
        end_line: symbol.endLine,
        end_column: symbol.endColumn,
      }));

      // For modified symbols, delete old versions and insert new
      for (const symbol of diff.modified) {
        const oldSymbol = this.symbolDAO.findByFileAndName(existingFile.id, symbol.name);
        if (oldSymbol) {
          this.chunkDAO.deleteBySymbolId(oldSymbol.id);
          this.symbolDAO.delete(oldSymbol.id);
        }
      }
      const modifiedIds = this.symbolDAO.insertBatch(modifiedInputs);

      // Create symbol ID map for chunks
      const symbolIdMap = new Map<number, number>();
      diff.added.forEach((symbol, idx) => {
        symbolIdMap.set(symbol.line, addedIds[idx]);
      });
      diff.modified.forEach((symbol, idx) => {
        symbolIdMap.set(symbol.line, modifiedIds[idx]);
      });

      // Insert chunks only for added/modified symbols
      const relevantLines = new Set([
        ...diff.added.map(s => s.line),
        ...diff.modified.map(s => s.line),
      ]);

      const chunkInputs: ChunkInput[] = chunkingResult.chunks
        .filter(chunk => relevantLines.has(chunk.start_line))
        .map(chunk => ({
          file_id: existingFile.id,
          content: chunk.content,
          start_line: chunk.start_line,
          end_line: chunk.end_line,
          chunk_type: chunk.chunk_type,
          symbol_id: symbolIdMap.get(chunk.start_line),
        }));

      this.chunkDAO.insertBatch(chunkInputs);

      return {
        fileId: existingFile.id,
        symbolCount: parseResult.symbols.length,
        chunkCount: chunkInputs.length,
        parseTime: parseResult.parseTime,
      };
    });

    const endTime = performance.now();

    // Invalidate cache
    this.invalidateCache();

    return {
      ...result,
      totalTime: endTime - startTime,
    };
  }
}
```

#### 3.3: Testing

**File:** `src/__tests__/services/IncrementalAnalyzer.test.ts` (NEW)

```typescript
import { describe, it, expect, beforeEach } from 'vitest';
import { IncrementalAnalyzer } from '../../services/IncrementalAnalyzer.js';
import { FileDAO } from '../../database/dao/FileDAO.js';
import { SymbolDAO } from '../../database/dao/SymbolDAO.js';
import { getTestDatabase } from '../helpers/testDatabase.js';

describe('IncrementalAnalyzer', () => {
  let analyzer: IncrementalAnalyzer;
  let fileDAO: FileDAO;
  let symbolDAO: SymbolDAO;

  beforeEach(() => {
    const db = getTestDatabase();
    fileDAO = new FileDAO(db);
    symbolDAO = new SymbolDAO(db);
    analyzer = new IncrementalAnalyzer(fileDAO, symbolDAO);
  });

  it('should detect added function', () => {
    // Setup: file with one function
    const fileId = fileDAO.insert({
      path: '/test.ts',
      content: 'function foo() {}',
      language: 'typescript',
    });

    symbolDAO.insert({
      file_id: fileId,
      name: 'foo',
      kind: 'function',
      line: 1,
      column: 0,
    });

    // New content with two functions
    const newContent = 'function foo() {}\nfunction bar() {}';
    const newSymbols = [
      { name: 'foo', kind: 'function', line: 1, column: 0 },
      { name: 'bar', kind: 'function', line: 2, column: 0 },
    ];

    const result = analyzer.analyzeChanges('/test.ts', newContent, newSymbols);

    expect(result.needsFullReindex).toBe(false);
    expect(result.diff?.added).toHaveLength(1);
    expect(result.diff?.added[0].name).toBe('bar');
    expect(result.diff?.removed).toHaveLength(0);
    expect(result.diff?.modified).toHaveLength(0);
  });

  it('should detect modified function', () => {
    const fileId = fileDAO.insert({
      path: '/test.ts',
      content: 'function foo() { return 1; }',
      language: 'typescript',
    });

    symbolDAO.insert({
      file_id: fileId,
      name: 'foo',
      kind: 'function',
      line: 1,
      column: 0,
    });

    // Modified function body
    const newContent = 'function foo() { return 2; }';
    const newSymbols = [
      { name: 'foo', kind: 'function', line: 1, column: 0 },
    ];

    const result = analyzer.analyzeChanges('/test.ts', newContent, newSymbols);

    expect(result.needsFullReindex).toBe(false);
    expect(result.diff?.modified).toHaveLength(1);
    expect(result.diff?.modified[0].name).toBe('foo');
  });

  it('should trigger full reindex for major changes', () => {
    const fileId = fileDAO.insert({
      path: '/test.ts',
      content: 'function foo() {}\nfunction bar() {}',
      language: 'typescript',
    });

    symbolDAO.insert({
      file_id: fileId,
      name: 'foo',
      kind: 'function',
      line: 1,
      column: 0,
    });
    symbolDAO.insert({
      file_id: fileId,
      name: 'bar',
      kind: 'function',
      line: 2,
      column: 0,
    });

    // Major refactoring - all new functions
    const newContent = 'function baz() {}\nfunction qux() {}\nfunction quux() {}';
    const newSymbols = [
      { name: 'baz', kind: 'function', line: 1, column: 0 },
      { name: 'qux', kind: 'function', line: 2, column: 0 },
      { name: 'quux', kind: 'function', line: 3, column: 0 },
    ];

    const result = analyzer.analyzeChanges('/test.ts', newContent, newSymbols);

    expect(result.needsFullReindex).toBe(true);
  });
});
```

**Expected Results:**
- 5-10x speedup for single function changes
- Accurate detection of added/removed/modified functions
- Proper fallback to full reindex for major changes

---

### Phase 4: Memory Optimization (Day 6)

**Objective:** Reduce memory usage by 50% for large projects

#### 4.1: Streaming File Processing

**File:** `src/services/StreamingIndexer.ts` (NEW)

```typescript
import * as fs from 'fs';
import * as readline from 'readline';
import type { ChunkInput } from '../database/dao/ChunkDAO.js';

export interface StreamingOptions {
  chunkSizeLines?: number;  // Default: 50 lines per chunk
  batchSize?: number;       // Default: 100 chunks per batch insert
}

/**
 * Streams large files line-by-line to reduce memory usage
 */
export class StreamingIndexer {
  constructor(private options: StreamingOptions = {}) {}

  /**
   * Process file in streaming mode
   * Yields chunks as they are created instead of buffering all
   */
  async *streamFileChunks(
    filePath: string,
    fileId: number
  ): AsyncGenerator<ChunkInput[]> {
    const chunkSizeLines = this.options.chunkSizeLines || 50;
    const batchSize = this.options.batchSize || 100;

    const fileStream = fs.createReadStream(filePath);
    const rl = readline.createInterface({
      input: fileStream,
      crlfDelay: Infinity,
    });

    let currentChunk: string[] = [];
    let currentStartLine = 1;
    let lineNumber = 1;
    let batch: ChunkInput[] = [];

    for await (const line of rl) {
      currentChunk.push(line);

      // If chunk is full, create chunk input
      if (currentChunk.length >= chunkSizeLines) {
        batch.push({
          file_id: fileId,
          content: currentChunk.join('\n'),
          start_line: currentStartLine,
          end_line: lineNumber,
          chunk_type: 'code_block',
        });

        // Reset chunk with overlap (last 10 lines for context)
        const overlapLines = Math.min(10, currentChunk.length);
        currentChunk = currentChunk.slice(-overlapLines);
        currentStartLine = lineNumber - overlapLines + 1;

        // Yield batch if full
        if (batch.length >= batchSize) {
          yield batch;
          batch = [];
        }
      }

      lineNumber++;
    }

    // Process remaining chunk
    if (currentChunk.length > 0) {
      batch.push({
        file_id: fileId,
        content: currentChunk.join('\n'),
        start_line: currentStartLine,
        end_line: lineNumber - 1,
        chunk_type: 'code_block',
      });
    }

    // Yield final batch
    if (batch.length > 0) {
      yield batch;
    }
  }
}
```

#### 4.2: Integrate Streaming into FileService

**File:** `src/services/FileService.ts` (MODIFY)

```typescript
import { StreamingIndexer } from './StreamingIndexer.js';

export class FileService {
  private streamingIndexer: StreamingIndexer;

  constructor(options?: {
    enableParallel?: boolean;
    enableIncremental?: boolean;
    enableStreaming?: boolean;
  }) {
    // ... existing initialization ...

    this.streamingIndexer = new StreamingIndexer({
      chunkSizeLines: 50,
      batchSize: 100,
    });
  }

  /**
   * Index large file using streaming mode
   * Use for files > 1MB to reduce memory usage
   */
  async indexLargeFile(path: string): Promise<IndexResult> {
    const startTime = performance.now();
    const stats = await fs.stat(path);

    // For small files, use regular indexing
    if (stats.size < 1024 * 1024) {
      const content = await fs.readFile(path, 'utf-8');
      return this.indexFile(path, content);
    }

    // For large files, use streaming
    const content = await fs.readFile(path, 'utf-8');
    const parseResult = this.parserRegistry.parse(content, path);

    const result = await transaction(async () => {
      // Insert file record
      const fileId = this.fileDAO.insert({
        path,
        content: '', // Don't store large file content
        language: this.parserRegistry.getLanguageForPath(path),
      });

      // Insert symbols (batch)
      const symbolInputs: SymbolInput[] = parseResult.symbols.map(symbol => ({
        file_id: fileId,
        name: symbol.name,
        kind: symbol.kind,
        line: symbol.line,
        column: symbol.column,
        end_line: symbol.endLine,
        end_column: symbol.endColumn,
      }));
      this.symbolDAO.insertBatch(symbolInputs);

      // Stream chunks in batches
      let totalChunks = 0;
      for await (const chunkBatch of this.streamingIndexer.streamFileChunks(path, fileId)) {
        this.chunkDAO.insertBatch(chunkBatch);
        totalChunks += chunkBatch.length;
      }

      return {
        fileId,
        symbolCount: parseResult.symbols.length,
        chunkCount: totalChunks,
        parseTime: parseResult.parseTime,
      };
    });

    const endTime = performance.now();

    return {
      ...result,
      totalTime: endTime - startTime,
    };
  }
}
```

#### 4.3: Memory Profiling Tests

**File:** `src/__tests__/performance/memory-usage.test.ts` (NEW)

```typescript
import { describe, it, expect } from 'vitest';
import { FileService } from '../../services/FileService.js';
import * as fs from 'fs/promises';

describe('Memory Usage', () => {
  it('should handle large files with bounded memory', async () => {
    const fileService = new FileService({ enableStreaming: true });

    // Create large test file (1MB+)
    const largeContent = 'function test() {}\n'.repeat(20000); // ~1.2MB
    await fs.writeFile('/tmp/large-file.ts', largeContent);

    // Measure memory before
    const memBefore = process.memoryUsage().heapUsed;

    // Index large file
    await fileService.indexLargeFile('/tmp/large-file.ts');

    // Force GC if available
    if (global.gc) {
      global.gc();
    }

    // Measure memory after
    const memAfter = process.memoryUsage().heapUsed;
    const memIncrease = (memAfter - memBefore) / 1024 / 1024; // MB

    console.log(`Memory increase: ${memIncrease.toFixed(2)} MB`);

    // Should use <50MB for 1.2MB file
    expect(memIncrease).toBeLessThan(50);

    // Cleanup
    await fs.unlink('/tmp/large-file.ts');
  });

  it('should handle 1000 files with <500MB', async () => {
    const fileService = new FileService({ enableStreaming: true });

    // Create test files
    const testDir = '/tmp/memory-test';
    await fs.mkdir(testDir, { recursive: true });

    for (let i = 0; i < 1000; i++) {
      await fs.writeFile(
        `${testDir}/file${i}.ts`,
        `export function test${i}() { return ${i}; }`
      );
    }

    // Measure memory
    const memBefore = process.memoryUsage().heapUsed;

    // Index all files
    const files = [];
    for (let i = 0; i < 1000; i++) {
      const content = await fs.readFile(`${testDir}/file${i}.ts`, 'utf-8');
      files.push({ path: `${testDir}/file${i}.ts`, content });
    }

    await fileService.indexFilesParallel(files);

    if (global.gc) {
      global.gc();
    }

    const memAfter = process.memoryUsage().heapUsed;
    const memUsage = memAfter / 1024 / 1024; // MB

    console.log(`Total memory usage: ${memUsage.toFixed(2)} MB`);

    // Should use <500MB for 1000 files
    expect(memUsage).toBeLessThan(500);

    // Cleanup
    await fs.rm(testDir, { recursive: true });
  });
});
```

**Run with:**
```bash
node --expose-gc node_modules/.bin/vitest src/__tests__/performance/memory-usage.test.ts
```

**Expected Results:**
- <50MB memory overhead for 1MB+ files
- <500MB total usage for 1000 files
- Bounded memory growth regardless of file size

---

### Phase 5: Performance Testing & Validation (Day 7)

**Objective:** Comprehensive benchmarks and performance regression suite

#### 5.1: Performance Benchmark Suite

**File:** `src/__tests__/performance/benchmark.test.ts` (NEW)

```typescript
import { describe, it, expect, beforeAll, afterAll } from 'vitest';
import { FileService } from '../../services/FileService.js';
import { QualityService } from '../../analytics/quality/QualityService.js';
import * as fs from 'fs/promises';
import * as path from 'path';

describe('Performance Benchmarks', () => {
  let fileService: FileService;
  let qualityService: QualityService;

  beforeAll(() => {
    fileService = new FileService({
      enableParallel: true,
      enableIncremental: true,
      enableStreaming: true,
    });
    qualityService = new QualityService({ enableParallel: true });
  });

  afterAll(async () => {
    await fileService.shutdown();
    await qualityService.shutdown();
  });

  it('should index 100 files in <5 seconds', async () => {
    const files = Array.from({ length: 100 }, (_, i) => ({
      path: `/test/file${i}.ts`,
      content: `
        export function test${i}(x: number) {
          if (x > 0) {
            return x * 2;
          }
          return 0;
        }
      `,
    }));

    const start = performance.now();
    await fileService.indexFilesParallel(files);
    const time = performance.now() - start;

    console.log(`Indexed 100 files in ${time.toFixed(2)}ms`);
    expect(time).toBeLessThan(5000);
  });

  it('should achieve >20 files/sec indexing speed', async () => {
    const files = Array.from({ length: 50 }, (_, i) => ({
      path: `/test/speed${i}.ts`,
      content: `
        export class Test${i} {
          private value = ${i};

          getValue(): number {
            return this.value;
          }

          setValue(v: number): void {
            this.value = v;
          }
        }
      `,
    }));

    const start = performance.now();
    await fileService.indexFilesParallel(files);
    const time = performance.now() - start;

    const filesPerSec = (files.length / time) * 1000;
    console.log(`Indexing speed: ${filesPerSec.toFixed(2)} files/sec`);

    expect(filesPerSec).toBeGreaterThan(20);
  });

  it('should achieve 80%+ cache hit rate', async () => {
    // Index files
    await fileService.indexFile('/test/cache1.ts', 'export const x = 1;');
    await fileService.indexFile('/test/cache2.ts', 'export const y = 2;');

    // Clear query cache stats
    fileService.clearCache();

    // Execute repeated queries
    for (let i = 0; i < 10; i++) {
      fileService.search('export');
      fileService.search('const');
    }

    const stats = fileService.getCacheStats();
    const hitRate = stats.hitRate * 100;

    console.log(`Cache hit rate: ${hitRate.toFixed(2)}%`);
    expect(hitRate).toBeGreaterThan(80);
  });

  it('should analyze project in <10 seconds', async () => {
    const testDir = '/tmp/benchmark-project';
    await fs.mkdir(testDir, { recursive: true });

    // Create realistic project structure
    for (let i = 0; i < 30; i++) {
      await fs.writeFile(
        path.join(testDir, `component${i}.ts`),
        `
          export class Component${i} {
            private state = { value: ${i} };

            render() {
              if (this.state.value > 0) {
                return this.renderContent();
              }
              return null;
            }

            private renderContent() {
              return this.state.value.toString();
            }
          }
        `
      );
    }

    const start = performance.now();
    const report = await qualityService.analyzeProject(testDir, ['typescript']);
    const time = performance.now() - start;

    console.log(`Analyzed ${report.fileReports.length} files in ${time.toFixed(2)}ms`);
    expect(time).toBeLessThan(10000);
    expect(report.fileReports.length).toBe(30);

    // Cleanup
    await fs.rm(testDir, { recursive: true });
  });

  it('should handle incremental reindex 5x faster', async () => {
    const originalContent = `
      export function foo() {
        return 1;
      }
      export function bar() {
        return 2;
      }
    `;

    // Initial index
    fileService.indexFile('/test/incremental.ts', originalContent);

    // Modify one function
    const modifiedContent = `
      export function foo() {
        return 42;  // Changed
      }
      export function bar() {
        return 2;
      }
    `;

    // Measure incremental reindex
    const start = performance.now();
    fileService.reindexFile('/test/incremental.ts', modifiedContent);
    const incrementalTime = performance.now() - start;

    // Measure full reindex (new file)
    const fullStart = performance.now();
    fileService.indexFile('/test/full.ts', modifiedContent);
    const fullTime = performance.now() - fullStart;

    const speedup = fullTime / incrementalTime;
    console.log(`Incremental speedup: ${speedup.toFixed(2)}x`);

    expect(speedup).toBeGreaterThan(2);
  });
});
```

#### 5.2: Performance Regression Suite

**File:** `src/__tests__/performance/regression.test.ts` (NEW)

```typescript
import { describe, it, expect } from 'vitest';
import { FileService } from '../../services/FileService.js';
import { performance } from 'perf_hooks';

/**
 * Performance regression tests
 * These ensure optimizations don't regress over time
 */
describe('Performance Regression', () => {
  // Baseline metrics (from P0)
  const BASELINE_METRICS = {
    indexing100Files: 10000, // 10 seconds
    queryLatencyCached: 1,   // 1ms
    queryLatencyUncached: 5, // 5ms
    cacheHitRate: 0.6,       // 60%
  };

  // Target metrics (P1 goals)
  const TARGET_METRICS = {
    indexing100Files: 5000,  // 5 seconds (2x improvement)
    queryLatencyCached: 1,   // 1ms (maintain)
    queryLatencyUncached: 3, // 3ms (1.7x improvement)
    cacheHitRate: 0.8,       // 80% (+33%)
  };

  it('should not regress indexing performance', async () => {
    const fileService = new FileService({ enableParallel: true });

    const files = Array.from({ length: 100 }, (_, i) => ({
      path: `/test/regression${i}.ts`,
      content: `export const value${i} = ${i};`,
    }));

    const start = performance.now();
    await fileService.indexFilesParallel(files);
    const time = performance.now() - start;

    console.log(`Indexing time: ${time.toFixed(2)}ms (target: <${TARGET_METRICS.indexing100Files}ms)`);

    // Should meet or beat target
    expect(time).toBeLessThan(TARGET_METRICS.indexing100Files);

    // Should definitely not regress to baseline
    expect(time).toBeLessThan(BASELINE_METRICS.indexing100Files);

    await fileService.shutdown();
  });

  it('should not regress cache performance', async () => {
    const fileService = new FileService();

    // Index test data
    fileService.indexFile('/test/cache.ts', 'export const x = 1;');

    // Warm up cache
    fileService.search('export');
    fileService.clearCache();

    // Measure cache hit rate
    fileService.search('export');
    fileService.search('export');
    fileService.search('export');

    const stats = fileService.getCacheStats();

    console.log(`Cache hit rate: ${(stats.hitRate * 100).toFixed(2)}% (target: ${TARGET_METRICS.cacheHitRate * 100}%)`);

    // Should meet or beat target
    expect(stats.hitRate).toBeGreaterThanOrEqual(TARGET_METRICS.cacheHitRate);
  });

  it('should not regress query latency', async () => {
    const fileService = new FileService();

    fileService.indexFile('/test/query.ts', 'export function test() {}');

    // Measure uncached query
    fileService.clearCache();
    const uncachedStart = performance.now();
    fileService.search('test');
    const uncachedTime = performance.now() - uncachedStart;

    console.log(`Uncached query: ${uncachedTime.toFixed(2)}ms (target: <${TARGET_METRICS.queryLatencyUncached}ms)`);
    expect(uncachedTime).toBeLessThan(TARGET_METRICS.queryLatencyUncached);

    // Measure cached query
    const cachedStart = performance.now();
    fileService.search('test');
    const cachedTime = performance.now() - cachedStart;

    console.log(`Cached query: ${cachedTime.toFixed(2)}ms (target: <${TARGET_METRICS.queryLatencyCached}ms)`);
    expect(cachedTime).toBeLessThan(TARGET_METRICS.queryLatencyCached);
  });
});
```

#### 5.3: CI Integration

**File:** `.github/workflows/performance.yml` (NEW)

```yaml
name: Performance Tests

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  performance:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Run performance tests
        run: |
          npm test -- src/__tests__/performance/benchmark.test.ts --reporter=verbose
          npm test -- src/__tests__/performance/regression.test.ts --reporter=verbose

      - name: Check for regressions
        run: |
          # Fail CI if performance regresses by >20%
          node scripts/check-performance-regression.js
```

---

## Success Metrics & Validation

### Quantitative Metrics

| Metric | Baseline (P0) | Target (P1) | Measurement Method |
|--------|--------------|-------------|-------------------|
| **Indexing Speed** | ~10 files/sec | >20 files/sec | Time to index 100 files  100 |
| **Parallel Speedup** | 1x (sequential) | 2-3x | Sequential time  Parallel time |
| **Cache Hit Rate** | 60% | 80% | Cached queries  Total queries |
| **Query Latency (P95 uncached)** | <5ms | <3ms | P95 of uncached query times |
| **Query Latency (cached)** | <1ms | <1ms | Average cached query time |
| **Memory Usage (1000 files)** | ~200MB | <500MB | Heap size after indexing 1000 files |
| **Incremental Speedup** | 1x (full reindex) | 5-10x | Full reindex time  Incremental time |
| **Test Coverage** | 100% (226 tests) | 100% | All tests passing |

### Qualitative Metrics

- **Code Quality:** All new code follows existing patterns
- **Test Quality:** Comprehensive unit, integration, and performance tests
- **Documentation:** Inline comments, JSDoc, and README updates
- **Backwards Compatibility:** All existing APIs unchanged

### Validation Checklist

- [ ] All benchmark tests passing
- [ ] All regression tests passing
- [ ] No memory leaks detected
- [ ] Worker threads properly terminated
- [ ] Cache eviction working correctly
- [ ] Incremental analysis accuracy >95%
- [ ] Parallel processing no race conditions
- [ ] All 226 existing tests still passing
- [ ] Performance metrics documented
- [ ] CI/CD integration complete

---

## Risk Mitigation

### Technical Risks

1. **Worker Thread Overhead**
   - Risk: Thread creation overhead exceeds parsing time for small files
   - Mitigation: Only use parallel processing for >20 files or file size >100KB
   - Fallback: Sequential processing for small batches

2. **Cache Invalidation Bugs**
   - Risk: Stale cache entries return incorrect results
   - Mitigation: Content hash-based keys, TTL expiration, file change detection
   - Testing: Cache invalidation test suite

3. **Incremental Analysis False Negatives**
   - Risk: Missing changes due to incorrect diff detection
   - Mitigation: Conservative thresholds (>30% change  full reindex)
   - Testing: Extensive diff detection tests

4. **Memory Fragmentation**
   - Risk: Streaming doesn't reduce memory due to fragmentation
   - Mitigation: Batch size tuning, periodic GC triggers
   - Testing: Memory profiling with large files

### Operational Risks

1. **Performance Regression**
   - Risk: Future changes slow down optimized code
   - Mitigation: Performance regression test suite in CI
   - Monitoring: Benchmark results tracked over time

2. **Complexity**
   - Risk: Increased code complexity makes maintenance harder
   - Mitigation: Clear documentation, separation of concerns
   - Review: Code review checklist for performance changes

---

## Implementation Timeline

| Day | Task | Deliverables | Validation |
|-----|------|-------------|-----------|
| **1** | Worker Pool Infrastructure | WorkerPool.ts, parsingWorker.ts | Unit tests passing |
| **2** | Parallel Integration | FileService + QualityService updates | Performance tests 2x+ speedup |
| **3** | AST Caching | ASTCache.ts, ParserRegistry updates | Cache hit rate 50%+ |
| **4** | Incremental Analysis Design | IncrementalAnalyzer.ts | Diff detection tests passing |
| **5** | Incremental Integration | FileService.reindexFileIncremental() | 5x+ speedup for small changes |
| **6** | Memory Optimization | StreamingIndexer.ts | <500MB for 1000 files |
| **7** | Testing & Validation | Benchmark + regression suites | All metrics met, CI green |

**Total: 7 days**

---

## Post-Implementation

### Documentation Updates

1. **README.md:**
   - Add performance benchmarks section
   - Update configuration options (enableParallel, enableCache, etc.)
   - Add troubleshooting section for worker threads

2. **API Documentation:**
   - Document new methods: `indexFilesParallel()`, `indexLargeFile()`
   - Document configuration options
   - Add performance tuning guide

3. **Architecture Docs:**
   - Update architecture diagram with worker threads
   - Document caching strategy
   - Document incremental analysis algorithm

### Monitoring & Observability

1. **Performance Metrics:**
   - Add telemetry for indexing time
   - Track cache hit rates
   - Monitor worker pool utilization

2. **Logging:**
   - Log worker pool stats on shutdown
   - Log cache stats periodically
   - Log incremental vs full reindex decisions

### Future Optimizations (P2+)

1. **Distributed Indexing:**
   - Multiple machines for very large codebases
   - Redis-based distributed cache
   - Estimated impact: 10x+ speedup for 100K+ files

2. **GPU-Accelerated Parsing:**
   - Tree-sitter GPU backend (experimental)
   - CUDA-accelerated text search
   - Estimated impact: 5-10x speedup for parsing

3. **ML-Based Change Prediction:**
   - Predict likely-to-change files for preemptive caching
   - Intelligent cache eviction based on access patterns
   - Estimated impact: 90%+ cache hit rate

---

## Conclusion

This performance optimization plan provides a clear, actionable roadmap to achieve 2-3x improvements in AutomatosX v2's indexing speed, memory efficiency, and query latency. The phased approach ensures each optimization is independently tested and validated before moving to the next.

**Key Highlights:**
- **Parallel Processing:** 2-3x indexing speedup via worker threads
- **AST Caching:** 50% reduction in repeated parsing overhead
- **Incremental Analysis:** 5-10x speedup for small code changes
- **Memory Optimization:** 50% memory reduction for large projects
- **Comprehensive Testing:** Benchmark + regression suites ensure no performance regressions

**Ready to Execute:** All design decisions made, code examples provided, testing strategies defined. Implementation can begin immediately.

---

**Next Steps:**
1. Review and approve this plan
2. Begin Day 1 implementation (Worker Pool Infrastructure)
3. Track progress against timeline and metrics
4. Adjust as needed based on real-world performance data

**Report Generated:** November 9, 2025
**Status:** Ready for Implementation
