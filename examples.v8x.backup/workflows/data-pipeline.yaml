# Data Pipeline Workflow
# ETL workflow with data validation and transformation

name: data-pipeline-workflow
description: ETL pipeline with validation, transformation, and quality checks
version: 1.0.0
author: AutomatosX
tags:
  - data
  - etl
  - pipeline

config:
  timeout: 600000  # 10 minutes
  maxRetries: 3
  parallelism: 2
  continueOnError: true

steps:
  # Step 1: Extract data from source
  - key: extract-data
    agent: data
    prompt: |
      Extract data from source:

      Source: {{source}}
      Date Range: {{dateRange}}

      Extract all records and return JSON array with:
      - record_id
      - timestamp
      - data fields

      Handle pagination if needed.
    dependencies: []
    parallel: false
    optional: false
    timeoutMs: 120000
    retryPolicy:
      maxRetries: 3
      retryDelayMs: 5000
      retryBackoffMultiplier: 2.0

  # Step 2: Validate data quality
  - key: validate-data
    agent: data
    prompt: |
      Validate data quality:

      Data: {{extract-data}}

      Check for:
      - Missing required fields
      - Data type validation
      - Value range validation
      - Duplicate records
      - Referential integrity

      Return JSON with:
      - validRecords: array of valid records
      - invalidRecords: array with validation errors
      - stats: counts and percentages
    dependencies:
      - extract-data
    parallel: false
    optional: false
    timeoutMs: 60000

  # Step 3: Transform data (parallel transformations)
  - key: transform-schema
    agent: data
    prompt: |
      Transform data schema:

      Valid Records: {{validate-data.validRecords}}

      Apply transformations:
      - Map old schema to new schema
      - Convert data types
      - Apply business rules
      - Add computed fields

      Return transformed records in target schema.
    dependencies:
      - validate-data
    parallel: true
    optional: false
    timeoutMs: 90000

  # Step 4: Enrich data (optional enrichment)
  - key: enrich-data
    agent: data
    prompt: |
      Enrich data with additional information:

      Transformed Data: {{transform-schema}}

      Add enrichments:
      - Geographic data (city, region)
      - Category classifications
      - External API data (if available)

      Return enriched records.
    dependencies:
      - transform-schema
    parallel: true
    optional: true  # Enrichment is optional
    timeoutMs: 120000

  # Step 5: Load to destination
  - key: load-data
    agent: data
    prompt: |
      Load data to destination:

      Data: {{transform-schema}}
      Enriched Data: {{enrich-data}}

      Destination: {{destination}}

      Load strategy:
      - Upsert (insert or update)
      - Batch size: 1000
      - Transaction per batch

      Return load summary:
      - recordsInserted
      - recordsUpdated
      - recordsFailed
      - duration
    dependencies:
      - transform-schema
      - enrich-data
    parallel: false
    optional: false
    timeoutMs: 180000
    retryPolicy:
      maxRetries: 3
      retryDelayMs: 10000
      retryBackoffMultiplier: 2.0

  # Step 6: Generate pipeline report
  - key: generate-report
    agent: data
    prompt: |
      Generate data pipeline execution report:

      Extraction: {{extract-data}}
      Validation: {{validate-data}}
      Load Summary: {{load-data}}

      Create report with:
      1. Pipeline status (success/partial/failed)
      2. Data quality metrics
      3. Transformation statistics
      4. Load performance
      5. Errors and warnings
      6. Next actions

      Return markdown formatted report.
    dependencies:
      - extract-data
      - validate-data
      - load-data
    parallel: false
    optional: false
    timeoutMs: 30000
